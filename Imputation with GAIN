
##### Creatung the GAIN myself #### GAIN Imputation Architecture and Confuiguration
##  NB that the code might change depending in the variables 
## My data contains mixed variable types (numerical and categorical)

## If not already available, install the packages below
!pip install torch scikit-learn tensorflow  keras

###### Creating GAIN for Imputations

#### #####
!pip install keras torch tensorflow 
import pandas as pd
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from sklearn.preprocessing import MinMaxScaler, OneHotEncoder
from typing import Tuple, Dict, List

# ----------------------
# Utility functions
# ----------------------

def sample_z(shape: Tuple[int, ...]) -> torch.Tensor:
    """Sample uniform noise in [0,1] for generator input."""
    return torch.rand(shape)

def sample_h(mask: torch.Tensor, hint_rate: float = 0.9) -> torch.Tensor:
    """
    Sample hint vector H: randomly reveal observed values with probability hint_rate.
    H = M * B + 0.5 * (1 - B), where B is a Bernoulli mask.
    """
    B = torch.rand_like(mask) < hint_rate
    return B.float() * mask + 0.5 * (1 - B.float())

# ----------------------
# Neural Networks
# ----------------------

class Generator(nn.Module):
    def __init__(self, input_dim: int, numeric_cols: List[str], categorical_dims: Dict[str, int]):
        super().__init__()
        self.input_dim = input_dim
        self.num_cols = len(numeric_cols)
        self.categorical_dims = categorical_dims
        self.output_dim = self.num_cols + sum(categorical_dims.values())
        
        # Shared layers with dropout for regularization
        self.shared = nn.Sequential(
            nn.Linear(input_dim * 2, 256),
            nn.LeakyReLU(0.2),
            nn.Dropout(0.1),
            nn.Linear(256, 128),
            nn.LeakyReLU(0.2),
            nn.Dropout(0.1)
        )
        
        # Numerical output
        self.numerical_out = nn.Sequential(
            nn.Linear(128, self.num_cols) if self.num_cols > 0 else nn.Identity(),
            nn.Sigmoid() if self.num_cols > 0 else nn.Identity()
        )
        
        # Categorical outputs
        self.categorical_out = nn.ModuleDict()
        for col, dim in categorical_dims.items():
            if dim == 1:  # Binary categorical
                self.categorical_out[col] = nn.Sequential(
                    nn.Linear(128, 1),
                    nn.Sigmoid()
                )
            else:  # Multi-class categorical
                self.categorical_out[col] = nn.Sequential(
                    nn.Linear(128, dim),
                    nn.Softmax(dim=1)
                )

    def forward(self, x: torch.Tensor, mask: torch.Tensor, z: torch.Tensor) -> torch.Tensor:
        assert x.shape[1] == self.input_dim, f"Input dim mismatch: expected {self.input_dim}, got {x.shape[1]}"
        
        x_input = x * mask + z * (1 - mask)
        shared_out = self.shared(torch.cat([x_input, mask], dim=1))
        
        # Numerical output
        num_out = self.numerical_out(shared_out)
        
        # Categorical outputs
        cat_outs = []
        for col in self.categorical_out:
            cat_out = self.categorical_out[col](shared_out)
            cat_outs.append(cat_out)
        
        output = torch.cat([num_out] + cat_outs, dim=1) if cat_outs else num_out
        assert output.shape[1] == self.output_dim, f"Output dim mismatch: expected {self.output_dim}, got {output.shape[1]}"
        return output

class Discriminator(nn.Module):
    def __init__(self, input_dim: int):
        super().__init__()
        self.model = nn.Sequential(
            nn.Linear(input_dim * 2, 256),
            nn.LeakyReLU(0.2),
            nn.Dropout(0.1),
            nn.Linear(256, 128),
            nn.LeakyReLU(0.2),
            nn.Dropout(0.1),
            nn.Linear(128, input_dim),
            nn.Sigmoid()
        )
    
    def forward(self, x_hat: torch.Tensor, hint: torch.Tensor) -> torch.Tensor:
        return self.model(torch.cat([x_hat, hint], dim=1))

# ----------------------
# Preprocessing
# ----------------------

def preprocess_data(df: pd.DataFrame) -> Tuple[pd.DataFrame, np.ndarray, Dict[str, MinMaxScaler], Dict[str, OneHotEncoder], List[str], List[str], Dict[str, int]]:
    df_proc = df.copy()
    scalers = {}
    encoders = {}
    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
    categorical_cols = [col for col in df.columns if df[col].dtype.name in ['object', 'category'] and df[col].dropna().nunique() >= 2]
    encoded_cols = numeric_cols.copy()
    categorical_dims = {}
    
    # Debug: Print column types and missingness
    print("Column dtypes:")
    print(df_proc.dtypes)
    print("Missingness rates:")
    print(df_proc.isna().mean())
    
    # Process numerical columns
    for col in numeric_cols:
        scaler = MinMaxScaler()
        values = df_proc[col].values.astype(float).reshape(-1, 1)
        mask = ~np.isnan(values).flatten()
        if mask.sum() > 0:
            clean_vals = values[mask].reshape(-1, 1)
            low, high = np.percentile(clean_vals, [1, 99])
            clipped_vals = np.clip(clean_vals, low, high)
            scaler.fit(clipped_vals)
            scaled = scaler.transform(values)
            scaled[~mask] = 0.5
            df_proc[col] = scaled.flatten()
        else:
            df_proc[col] = 0.5
        scalers[col] = scaler
    
    # Process categorical columns
    for col in categorical_cols:
        # Convert categorical to object to avoid CategoricalDtype restrictions
        df_proc[col] = df_proc[col].astype('object')
        df_proc[col] = df_proc[col].fillna('__MISSING__')
        encoder = OneHotEncoder(sparse_output=False, drop='first', handle_unknown='ignore')
        valid_data = df_proc[df_proc[col] != '__MISSING__'][[col]]
        if len(valid_data) == 0:
            raise ValueError(f"No valid data for categorical column {col}")
        encoder.fit(valid_data)
        transformed = encoder.transform(df_proc[[col]])
        col_names = [f"{col}_{cat}" for cat in encoder.categories_[0][1:]]
        encoded_cols.extend(col_names)
        encoders[col] = encoder
        categorical_dims[col] = len(col_names)
        df_proc = pd.concat([df_proc.drop(col, axis=1), 
                           pd.DataFrame(transformed, columns=col_names, index=df_proc.index)], axis=1)
    
    if not (numeric_cols or categorical_cols):
        raise ValueError("No numerical or categorical columns with at least two categories found.")
    
    data = df_proc[encoded_cols].values.astype(np.float32)
    mask = (~df_proc[encoded_cols].isna()).values.astype(np.float32)
    data[np.isnan(data)] = 0.5
    
    # Check data variability
    if data.std(axis=0).min() < 1e-6:
        print("Warning: Low variability in encoded data. Training may be unstable.")
    
    print(f"Preprocessed data shape: {data.shape}")
    print(f"Encoded columns: {encoded_cols}")
    print(f"Categorical dimensions: {categorical_dims}")
    return df_proc[encoded_cols], mask, scalers, encoders, numeric_cols, categorical_cols, categorical_dims

# ----------------------
# GAIN Model
# ----------------------

class GAIN:
    def __init__(self, data_dim: int, numeric_cols: List[str], categorical_dims: Dict[str, int], 
                 hidden_dim: int = 128, lr: float = 1e-4, alpha: float = 100, hint_rate: float = 0.9, grad_clip: float = 1.0):
        self.data_dim = data_dim
        self.numeric_cols = numeric_cols
        self.categorical_dims = categorical_dims
        self.alpha = alpha
        self.hint_rate = hint_rate
        self.grad_clip = grad_clip

        self.G = Generator(data_dim, numeric_cols, categorical_dims)
        self.D = Discriminator(data_dim)

        self.optim_G = optim.Adam(self.G.parameters(), lr=lr)
        self.optim_D = optim.Adam(self.D.parameters(), lr=lr)

        self.bce_loss = nn.BCELoss(reduction='none')

    def train(self, data: torch.Tensor, mask: torch.Tensor, batch_size: int = 128, epochs: int = 1000):
        N = data.shape[0]
        for epoch in range(epochs):
            perm = torch.randperm(N)
            d_loss_total = 0.0
            g_loss_total = 0.0
            batch_count = 0
            
            for i in range(0, N, batch_size):
                batch_idx = perm[i:i + batch_size]
                x = data[batch_idx]
                m = mask[batch_idx]
                z = sample_z(x.shape)
                h = sample_h(m, self.hint_rate)

                # Discriminator step
                self.optim_D.zero_grad()
                x_hat = self.G(x, m, z)
                x_completed = m * x + (1 - m) * x_hat
                d_prob = self.D(x_completed.detach(), h)
                d_loss = self.bce_loss(d_prob, m).mean(dim=1).sum()
                d_loss.backward()
                torch.nn.utils.clip_grad_norm_(self.D.parameters(), self.grad_clip)
                self.optim_D.step()

                # Generator step
                self.optim_G.zero_grad()
                x_hat = self.G(x, m, z)
                x_completed = m * x + (1 - m) * x_hat
                d_prob = self.D(x_completed, h)
                g_loss_adv = self.bce_loss(d_prob, torch.ones_like(d_prob)).mean(dim=1).sum()
                g_loss_rec = ((x_hat - x) ** 2 * m).sum() / (m.sum() + 1e-8)
                g_loss = g_loss_adv + self.alpha * g_loss_rec
                g_loss.backward()
                torch.nn.utils.clip_grad_norm_(self.G.parameters(), self.grad_clip)
                self.optim_G.step()

                d_loss_total += d_loss.item()
                g_loss_total += g_loss.item()
                batch_count += 1

            if (epoch + 1) % 100 == 0 or epoch == 0:
                print(f"Epoch [{epoch+1}/{epochs}] | D Loss: {d_loss_total/batch_count:.4f} | G Loss: {g_loss_total/batch_count:.4f}")

    def impute(self, data: torch.Tensor, mask: torch.Tensor) -> torch.Tensor:
        """Impute missing values using trained generator."""
        z = sample_z(data.shape)
        self.G.eval()
        with torch.no_grad():
            x_hat = self.G(data, mask, z)
            imputed_data = mask * data + (1 - mask) * x_hat
        self.G.train()
        return imputed_data

def gain_impute(df: pd.DataFrame, config: Dict = None) -> pd.DataFrame:
    if config is None:
        config = {
            'hint_rate': 0.9,
            'alpha': 100,
            'epochs': 300,
            'batch_size': 64,
            'lr': 1e-4,
            'hidden_dim': 128,
            'grad_clip': 1.0
        }
    
    df_proc, mask, scalers, encoders, numeric_cols, categorical_cols, categorical_dims = preprocess_data(df)
    data_tensor = torch.tensor(df_proc.values, dtype=torch.float32)
    mask_tensor = torch.tensor(mask, dtype=torch.float32)
    
    gain = GAIN(data_dim=data_tensor.shape[1], numeric_cols=numeric_cols, categorical_dims=categorical_dims,
                hidden_dim=config['hidden_dim'], lr=config['lr'], alpha=config['alpha'], 
                hint_rate=config['hint_rate'], grad_clip=config['grad_clip'])
    
    gain.train(data_tensor, mask_tensor, batch_size=config['batch_size'], epochs=config['epochs'])
    
    imputed_data = gain.impute(data_tensor, mask_tensor)
    df_imputed = pd.DataFrame(imputed_data.numpy(), columns=df_proc.columns)
    
    # Inverse transform numerical columns
    for col in numeric_cols:
        df_imputed[col] = scalers[col].inverse_transform(df_imputed[[col]].values)
    
    # Inverse transform categorical columns
    result_df = df.copy().reset_index(drop=True)
    for col in categorical_cols:
        cat_cols = [c for c in df_imputed.columns if c.startswith(f"{col}_")]
        cat_vals = df_imputed[cat_cols].values
        if len(cat_cols) == 1:  # Binary categorical
            values = cat_vals.flatten()
            categories = encoders[col].categories_[0]
            result_df[col] = np.where(values > 0.5, categories[1], categories[0])
        else:  # Multi-class categorical
            decoded = encoders[col].inverse_transform(cat_vals)
            result_df[col] = decoded.flatten()
    
    return result_df[numeric_cols + categorical_cols]


print(imputed_data)

### Check the distribution of one of the variables
imputed_data['sex'].value_counts()

