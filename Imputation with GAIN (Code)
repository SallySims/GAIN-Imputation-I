
##### Creatung the GAIN myself #### GAIN Imputation Architecture and Confuiguration
##  NB that the code might change depending in the variables 
## My data contains mixed variable types (numerical and categorical)

## If not already available, install the packages below
!pip install torch scikit-learn tensorflow  keras

###### Creating GAIN for Imputations
############### Creating the GAIN Imputer for the Study ##############
!pip install keras torch tensorflow
# Install necessary libraries (uncomment if running in a fresh environment)
import pandas as pd
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from sklearn.preprocessing import MinMaxScaler, OneHotEncoder
from typing import Tuple, List
from tqdm import tqdm

############# Numerical Data Imputations #######

# Creatung the Helper Functions
def sample_noise(shape: Tuple[int, ...], device: torch.device) -> torch.Tensor:
    return torch.rand(shape, device=device) * 0.01

def sample_hint(mask: torch.Tensor, hint_rate: float = 0.9) -> torch.Tensor:
    hint_prob = torch.rand_like(mask) < hint_rate
    return hint_prob.float() * mask + 0.5 * (1 - hint_prob.float())

# Creating the Generator
class Generator(nn.Module):
    def __init__(self, input_dim: int, hidden_dim: int = 128):
        super().__init__()
        self.model = nn.Sequential(
            nn.Linear(input_dim * 2, hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(hidden_dim // 2, input_dim),
            nn.Sigmoid()
        )
        self.apply(self._init_weights)
    
    def _init_weights(self, module):
        if isinstance(module, nn.Linear):
            nn.init.xavier_uniform_(module.weight)
            nn.init.constant_(module.bias, 0)
    
    def forward(self, x: torch.Tensor, mask: torch.Tensor, noise: torch.Tensor) -> torch.Tensor:
        x_input = x * mask + noise * (1 - mask)
        return self.model(torch.cat([x_input, mask], dim=1))

# Creatng the  Discriminator
class Discriminator(nn.Module):
    def __init__(self, input_dim: int, hidden_dim: int = 128):
        super().__init__()
        self.model = nn.Sequential(
            nn.Linear(input_dim * 2, hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(hidden_dim // 2, input_dim),
            nn.Sigmoid()
        )
        self.apply(self._init_weights)
    
    def _init_weights(self, module):
        if isinstance(module, nn.Linear):
            nn.init.xavier_uniform_(module.weight)
            nn.init.constant_(module.bias, 0)
    
    def forward(self, x_hat: torch.Tensor, hint: torch.Tensor) -> torch.Tensor:
        return self.model(torch.cat([x_hat, hint], dim=1))

#  Creating the Data Preprocessor Section
class DataPreprocessor:
    def __init__(self):
        self.scalers = {}
        self.numeric_cols = []
        self.original_columns = []
    
    def fit_transform(self, df: pd.DataFrame) -> Tuple[np.ndarray, np.ndarray, List[str]]:
        if df.empty:
            raise ValueError("Input DataFrame is empty")
        
        self.original_columns = df.columns.tolist()
        self.numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
        if not self.numeric_cols:
            raise ValueError("No numerical columns found in DataFrame")
        
        df_processed = df[self.numeric_cols].copy()
        
        # Process numerical columns
        for col in self.numeric_cols:
            scaler = MinMaxScaler(feature_range=(0.01, 0.99))
            values = df_processed[col].values.reshape(-1, 1).astype(np.float32)
            mask = ~np.isnan(values).flatten()
            if mask.sum() > 0:
                scaler.fit(values[mask].reshape(-1, 1))
                df_processed.loc[mask, col] = scaler.transform(values[mask].reshape(-1, 1)).flatten()
            else:
                scaler.fit(np.array([[0], [1]]))
                df_processed[col] = 0.5
            self.scalers[col] = scaler
        
        data = df_processed.values.astype(np.float32)
        mask = (~df_processed.isna()).values.astype(np.float32)
        data[np.isnan(data)] = 0.5
        
        return data, mask, self.numeric_cols
    
    def inverse_transform(self, data: np.ndarray, columns: List[str]) -> pd.DataFrame:
        df_result = pd.DataFrame(data, columns=columns, index=range(data.shape[0]))
        
        # Inverse transform numerical columns
        for col in self.numeric_cols:
            if col in self.scalers:
                df_result[col] = self.scalers[col].inverse_transform(df_result[[col]].values).flatten()
        
        return df_result[self.numeric_cols]

# Creating the Main GAIN Imputer
class GAINImputer:
    def __init__(self, data_dim: int, lr: float = 1e-3, alpha: float = 10, hint_rate: float = 0.9, hidden_dim: int = 128):
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.G = Generator(data_dim, hidden_dim).to(self.device)
        self.D = Discriminator(data_dim, hidden_dim).to(self.device)
        self.optim_G = optim.Adam(self.G.parameters(), lr=lr, betas=(0.5, 0.9))
        self.optim_D = optim.Adam(self.D.parameters(), lr=lr, betas=(0.5, 0.9))
        self.alpha = alpha
        self.hint_rate = hint_rate
        self.bce_loss = nn.BCELoss(reduction='none')
    
    def train(self, data: np.ndarray, mask: np.ndarray, batch_size: int = 64, epochs: int = 300):
        data_tensor = torch.FloatTensor(data).to(self.device)
        mask_tensor = torch.FloatTensor(mask).to(self.device)
        n_samples = data.shape[0]
        batches = (n_samples + batch_size - 1) // batch_size
        
        for epoch in tqdm(range(epochs), desc="Training GAIN"):
            perm = torch.randperm(n_samples)
            epoch_d_loss, epoch_g_loss = 0, 0
            
            for i in range(0, n_samples, batch_size):
                batch_idx = perm[i:min(i + batch_size, n_samples)]
                x, m = data_tensor[batch_idx], mask_tensor[batch_idx]
                noise = sample_noise(x.shape, self.device)
                hint = sample_hint(m, self.hint_rate).to(self.device)
                
                # Train Discriminator
                self.optim_D.zero_grad()
                x_hat = self.G(x, m, noise)
                x_completed = m * x + (1 - m) * x_hat
                d_prob = self.D(x_completed.detach(), hint)
                d_loss = self.bce_loss(d_prob, m).mean()
                d_loss.backward()
                self.optim_D.step()
                
                # Train Generator
                self.optim_G.zero_grad()
                x_hat = self.G(x, m, noise)
                x_completed = m * x + (1 - m) * x_hat
                d_prob = self.D(x_completed, hint)
                g_loss_adv = self.bce_loss(d_prob, torch.ones_like(d_prob)).mean()
                g_loss_rec = ((m * (x - x_hat)) ** 2).mean() / m.mean()
                g_loss = g_loss_adv + self.alpha * g_loss_rec
                g_loss.backward()
                self.optim_G.step()
                
                epoch_d_loss += d_loss.item()
                epoch_g_loss += g_loss.item()
            
            epoch_d_loss /= batches
            epoch_g_loss /= batches
            
            if (epoch + 1) % 10 == 0:
                print(f"Epoch {epoch + 1}/{epochs} - D Loss: {epoch_d_loss:.4f} - G Loss: {epoch_g_loss:.4f}")
    
    def impute(self, data: np.ndarray, mask: np.ndarray) -> np.ndarray:
        data_tensor = torch.FloatTensor(data).to(self.device)
        mask_tensor = torch.FloatTensor(mask).to(self.device)
        noise = sample_noise(data_tensor.shape, self.device)
        
        with torch.no_grad():
            x_hat = self.G(data_tensor, mask_tensor, noise)
            imputed_data = mask_tensor * data_tensor + (1 - mask_tensor) * x_hat
        
        return imputed_data.cpu().numpy()

# Main Imputation Function
def gain_impute_numerical(df: pd.DataFrame, 
                                batch_size: int = 64, epends on the size of data
                                epochs: int = 300, ### depends on the size of data
                                lr: float = 1e-3, epends on the size of data
                                alpha: float = 10, epends on the size of data
                                hint_rate: float = 0.9) -> pd.DataFrame:
    print(f"Starting GAIN imputation for numerical variables in DataFrame of shape {df.shape} with {df.isna().sum().sum()} missing values")
    
    preprocessor = DataPreprocessor()
    data, mask, encoded_cols = preprocessor.fit_transform(df)
    
    imputer = GAINImputer(data_dim=data.shape[1], lr=lr, alpha=alpha, hint_rate=hint_rate)
    imputer.train(data, mask, batch_size=batch_size, epochs=epochs)
    
    print("Generating imputed data...")
    imputed_data = imputer.impute(data, mask)
    
    print("Converting back to original format...")
    result_df = preprocessor.inverse_transform(imputed_data, encoded_cols)
    
    print(f"Imputation completed. Output shape: {result_df.shape}, remaining missing values: {result_df.isna().sum().sum()}")
    
    return result_df

############ Trail for Numerical Data Impuation 
### Check the distribution of a numerical variable
imputed_data['age'].value_counts()

########## Imputing with Numerical Variables #########
imputed_data =  gain_impute_numerical(SADat2[['pweight', 'age', 'height','weight', 'WC', 'HC', 'BPs']])
imputed_dfSA1=imputed_data.round(2)
imputed_dfSA1


######################################################################################################################
################  GAIN for Categorical variables 

########## GAIN for Categories### 

########## Categorical Data Imputation #########

# ========== Helper Functions ==========

def sample_z(shape: Tuple[int, ...]) -> torch.Tensor:
    return torch.rand(shape)

def sample_h(mask: torch.Tensor, hint_rate: float = 0.9) -> torch.Tensor:
    B = torch.rand_like(mask) < hint_rate
    return B.float() * mask + (1 - B.float()) * 0.5

# ========== Generator Network ==========

class Generator(nn.Module):
    def __init__(self, input_dim):
        super().__init__()
        self.model = nn.Sequential(
            nn.Linear(input_dim * 2, 256),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(256, 256),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(256, input_dim),
            nn.Sigmoid()
        )

    def forward(self, x, m):
        return self.model(torch.cat([x, m], dim=1))

# ========== Discriminator Network ==========

class Discriminator(nn.Module):
    def __init__(self, input_dim):
        super().__init__()
        self.model = nn.Sequential(
            nn.Linear(input_dim * 2, 256),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(256, 256),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(256, input_dim),
            nn.Sigmoid()
        )

    def forward(self, x, h):
        return self.model(torch.cat([x, h], dim=1))

# ========== Preprocessor for Categorical Variables ==========

class CategoricalPreprocessor:
    def __init__(self):
        self.encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')
        self.original_columns = []

    def fit_transform(self, df: pd.DataFrame) -> Tuple[np.ndarray, np.ndarray]:
        self.original_columns = df.columns.tolist()
        
        # Convert categorical dtype to object to allow "missing" placeholder
        for col in df.columns:
            if pd.api.types.is_categorical_dtype(df[col]):
                df[col] = df[col].astype('object')

        mask = ~df.isnull()
        df_filled = df.fillna("missing")

        encoded = self.encoder.fit_transform(df_filled)

        # Generate mask for each one-hot feature
        mask_array = np.concatenate([
            np.repeat(mask[[col]].astype(float).values, len(cats), axis=1)
            for col, cats in zip(self.original_columns, self.encoder.categories_)
        ], axis=1)

        return encoded.astype(np.float32), mask_array.astype(np.float32)

    def inverse_transform(self, data: np.ndarray) -> pd.DataFrame:
        decoded = self.encoder.inverse_transform(data)
        return pd.DataFrame(decoded, columns=self.original_columns)

# ========== GAIN Model for Categorical Data ==========

class GAIN:
    def __init__(self, data_dim, lr=1e-3, alpha=10, hint_rate=0.9):
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.G = Generator(data_dim).to(self.device)
        self.D = Discriminator(data_dim).to(self.device)
        self.optim_G = optim.Adam(self.G.parameters(), lr=lr)
        self.optim_D = optim.Adam(self.D.parameters(), lr=lr)
        self.alpha = alpha
        self.hint_rate = hint_rate

    def train(self, data, mask, batch_size=64, epochs=300):
        data_tensor = torch.tensor(data).to(self.device)
        mask_tensor = torch.tensor(mask).to(self.device)

        for epoch in range(epochs):
            indices = torch.randperm(len(data_tensor))[:batch_size]
            x = data_tensor[indices]
            m = mask_tensor[indices]
            z = sample_z(x.shape).to(self.device)
            x_input = m * x + (1 - m) * z
            h = sample_h(m, self.hint_rate).to(self.device)

            # ----- Discriminator Step -----
            with torch.no_grad():
                x_fake = self.G(x_input, m)
            x_hat = m * x + (1 - m) * x_fake
            d_prob = self.D(x_hat, h)
            d_loss = -torch.mean(m * torch.log(d_prob + 1e-8) +
                                 (1 - m) * torch.log(1 - d_prob + 1e-8))
            self.optim_D.zero_grad()
            d_loss.backward()
            self.optim_D.step()

            # ----- Generator Step -----
            x_fake = self.G(x_input, m)
            x_hat = m * x + (1 - m) * x_fake
            d_prob = self.D(x_hat, h)
            g_loss_adv = -torch.mean((1 - m) * torch.log(d_prob + 1e-8))
            g_loss_rec = torch.mean(m * (x - x_fake) ** 2) / torch.mean(m)
            g_loss = g_loss_adv + self.alpha * g_loss_rec
            self.optim_G.zero_grad()
            g_loss.backward()
            self.optim_G.step()

            if (epoch + 1) % 50 == 0:
                print(f"Epoch {epoch + 1}/{epochs} | D Loss: {d_loss.item():.4f} | G Loss: {g_loss.item():.4f}")

    def impute(self, data, mask):
        data_tensor = torch.tensor(data).to(self.device)
        mask_tensor = torch.tensor(mask).to(self.device)
        z = sample_z(data.shape).to(self.device)
        x_input = mask_tensor * data_tensor + (1 - mask_tensor) * z
        with torch.no_grad():
            x_fake = self.G(x_input, mask_tensor)
            x_imputed = mask_tensor * data_tensor + (1 - mask_tensor) * x_fake
        return x_imputed.cpu().numpy()

# ========== Final Function to Use ==========

def gain_impute_categorical(df: pd.DataFrame) -> pd.DataFrame:
    print("Preparing categorical data for GAIN imputation...")

    # Filter categorical columns only
    cat_df = df.select_dtypes(include=['object', 'category', 'bool']).copy()

    # Convert Categorical columns to object dtype
    for col in cat_df.columns:
        if pd.api.types.is_categorical_dtype(cat_df[col]):
            cat_df[col] = cat_df[col].astype('object')

    # Preprocess and encode
    preprocessor = CategoricalPreprocessor()
    data, mask = preprocessor.fit_transform(cat_df)

    # Train GAIN model
    gain = GAIN(data_dim=data.shape[1])
    gain.train(data, mask, batch_size=64, epochs=300)

    # Perform imputation
    imputed_data = gain.impute(data, mask)
    result = preprocessor.inverse_transform(imputed_data)

    print("Categorical imputation complete.")
    return result

              ####### Categorical Data ######
########## Imputing with the GAIN Imputer Variables #########
imputed_data2 =  gain_impute_categorical(SADat2.drop(columns=['Numerical Variables for retain only Categorical Variables']))

imputed_dfSA2=imputed_data2
imputed_dfSA2

######### Merging All the Data Types ###### Merging All the Data Types #######
imputed_dfSA=pd.concat([imputed_dfSA1, imputed_dfSA2], axis=1)
imputed_dfSA


